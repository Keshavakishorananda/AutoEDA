{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loaded 72 expert trajectories\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0batch [00:00, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA_gather)\n",
      "\n",
      "Debugging information:\n",
      "Policy device: cuda:0\n",
      "mlp_extractor.policy_net.0.weight: cuda:0\n",
      "mlp_extractor.policy_net.0.bias: cuda:0\n",
      "mlp_extractor.policy_net.2.weight: cuda:0\n",
      "mlp_extractor.policy_net.2.bias: cuda:0\n",
      "mlp_extractor.policy_net.4.weight: cuda:0\n",
      "mlp_extractor.policy_net.4.bias: cuda:0\n",
      "mlp_extractor.value_net.0.weight: cuda:0\n",
      "mlp_extractor.value_net.0.bias: cuda:0\n",
      "mlp_extractor.value_net.2.weight: cuda:0\n",
      "mlp_extractor.value_net.2.bias: cuda:0\n",
      "mlp_extractor.value_net.4.weight: cuda:0\n",
      "mlp_extractor.value_net.4.bias: cuda:0\n",
      "action_net.weight: cuda:0\n",
      "action_net.bias: cuda:0\n",
      "value_net.weight: cuda:0\n",
      "value_net.bias: cuda:0\n",
      "Initialization parameters saved to BC_Initialization/cyber_security/policy.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "from imitation.algorithms.bc import BC\n",
    "from imitation.algorithms.adversarial.gail import GAIL\n",
    "from imitation.rewards.reward_nets import BasicRewardNet\n",
    "from imitation.rewards import reward_wrapper\n",
    "from imitation.util import networks\n",
    "from imitation.scripts.train_adversarial import save as save_trainer\n",
    "from imitation.data.types import Trajectory\n",
    "\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "from stable_baselines3.ppo import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "\n",
    "from Environment import MultidatasetEnvironment\n",
    "\n",
    "cyber_security_datasets = [f'Datasets/Cyber_security/{i}.tsv' for i in range(1,5)]\n",
    "flight_delay_datasets = [f'Datasets/Flight_delay/{i}.tsv' for i in range(1,5)]\n",
    "\n",
    "\n",
    "def train_BC(expert_trajectories_path, datasets):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    with open(expert_trajectories_path, 'rb') as f:\n",
    "        expert_trajectories = pickle.load(f)\n",
    "\n",
    "    print(f\"Loaded {len(expert_trajectories)} expert trajectories\")\n",
    "\n",
    "    env = MultidatasetEnvironment(datasets)\n",
    "    \n",
    "    policy = ActorCriticPolicy(\n",
    "        env.observation_space, \n",
    "        env.action_space, \n",
    "        lr_schedule=lambda x: 1e-4, \n",
    "        net_arch=dict(pi=[100, 100, 100], vf=[100, 100, 100])\n",
    "    )\n",
    "    policy = policy.to(device)\n",
    "\n",
    "    rng = np.random.default_rng()\n",
    "\n",
    "    trainer = BC(\n",
    "        observation_space=env.observation_space, \n",
    "        action_space=env.action_space,\n",
    "        policy=policy,\n",
    "        demonstrations=expert_trajectories,\n",
    "        rng=rng,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        trainer.train(n_epochs=100)\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Training error: {str(e)}\")\n",
    "        # Additional debugging information\n",
    "        print(\"\\nDebugging information:\")\n",
    "        print(f\"Policy device: {next(trainer.policy.parameters()).device}\")\n",
    "        for name, param in trainer.policy.named_parameters():\n",
    "            print(f\"{name}: {param.device}\")\n",
    "            \n",
    "    # Save the trained policy\n",
    "    save_path = 'BC_Initialization/cyber_security'\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    torch.save(trainer.policy.state_dict(), f'{save_path}/policy.pth')\n",
    "    print('Initialization parameters saved to', f'{save_path}/policy.pth')\n",
    "\n",
    "\n",
    "def train_GAIL():\n",
    "    total_steps = 2000000\n",
    "    n_gen_updates_per_round = 1\n",
    "    \n",
    "    venv = DummyVecEnv([lambda:MultidatasetEnvironment(dataset_paths=cyber_security_datasets, max_steps= 12) for _ in range(8)])\n",
    "\n",
    "    # Generator\n",
    "    policy_kwargs = dict(net_arch = dict(pi = [100, 100, 100], vf = [100, 100, 100]))\n",
    "    learner = PPO(ActorCriticPolicy, venv, n_steps=48 , batch_size=32, policy_kwargs=policy_kwargs)\n",
    "\n",
    "    # Descrminator\n",
    "    env = MultidatasetEnvironment(cyber_security_datasets, 12)\n",
    "    reward_net = BasicRewardNet(env.observation_space, env.action_space)\n",
    "\n",
    "    # Load intialized parametrs to generator\n",
    "    bc_policy = torch.load(f'BC_Initialization/cyber_security/policy.pth')\n",
    "    learner.policy.load_state_dict(bc_policy)\n",
    "\n",
    "    # Get expert_trajectories\n",
    "    expert_trajectories_path = 'Expert_trajectories/train/1.pkl'\n",
    "    with open(expert_trajectories_path, 'rb') as f:\n",
    "        expert_trajectories = pickle.load(f)\n",
    "\n",
    "    trainer = GAIL(\n",
    "        demonstrations = expert_trajectories,\n",
    "        demo_batch_size= 96*2,\n",
    "        gen_replay_buffer_capacity= 768,\n",
    "        n_disc_updates_per_round= 2,\n",
    "        venv= venv,\n",
    "        gen_algo=learner,\n",
    "        reward_net=reward_net,\n",
    "        allow_variable_horizon=True,\n",
    "    )\n",
    "\n",
    "    print(trainer.gen_train_timesteps)\n",
    "\n",
    "    def get_custom_reward_function(trainer):\n",
    "        venv = trainer.venv_wrapped\n",
    "\n",
    "        def repeat_penalty(old_obs, acts, obs, dones):\n",
    "            return np.array([env.repeat_penalty() for env in venv.envs])\n",
    "\n",
    "        def custom_reward_function(old_obs, acts, obs, dones):\n",
    "            r1 = trainer.reward_train.predict_processed(old_obs, acts, obs, dones)\n",
    "            r2 = np.array(repeat_penalty(old_obs, acts, obs, dones))\n",
    "            return r1 + r2\n",
    "\n",
    "        return custom_reward_function\n",
    "\n",
    "    # r = get_custom_reward_function(trainer)\n",
    "    # trainer.venv_wrapped = reward_wrapper.RewardVecEnvWrapper(\n",
    "    #     trainer.venv_buffering,\n",
    "    #     reward_fn=r\n",
    "    # )\n",
    "    # trainer.venv_train = trainer.venv_wrapped\n",
    "    # trainer.gen_algo.set_env(trainer.venv_train)\n",
    "\n",
    "    n_rounds = total_steps // (n_gen_updates_per_round * trainer.gen_train_timesteps)\n",
    "    assert n_rounds >= 1, (\n",
    "        \"No updates (need at least \"\n",
    "        f\"{trainer.gen_train_timesteps} timesteps, have only \"\n",
    "        f\"total_timesteps={total_steps})!\"\n",
    "    )\n",
    "\n",
    "    # Training loop\n",
    "    for i in tqdm.tqdm(range(0, n_rounds), desc = \"round\"):\n",
    "        for _ in range(n_gen_updates_per_round):\n",
    "            trainer.train_gen(trainer.gen_train_timesteps)\n",
    "        for _ in range(trainer.n_disc_updates_per_round):\n",
    "            with networks.training(trainer.reward_train):\n",
    "                trainer.train_disc()\n",
    "\n",
    "    save_trainer(trainer, f'Final_Parameters/cyber_security.GAILAgent')    \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_BC('Expert_trajectories/train/1.pkl', cyber_security_datasets)\n",
    "    # train_GAIL()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
